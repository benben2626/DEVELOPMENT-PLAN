import numpy as np
from sklearn import preprocessing, neighbors
from sklearn.model_selection import train_test_split
import pandas as pd

df = pd.read_csv('breast-cancer-wisconsin.data.txt')
df.replace('?',-99999, inplace=True)
df.drop(['id'], 1, inplace=True) #other in first column become index #no statistic meaninig

X = np.array(df.drop(['class'], 1))
y = np.array(df['class'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf = neighbors.KNeighborsClassifier()
clf.fit(X_train, y_train)
accuracy = clf.score(X_test, y_test)
print(accuracy)

# one_sample
example_measures = np.array([4,2,1,1,1,2,3,2,1])
example_measures = example_measures.reshape(1, -1)
prediction = clf.predict(example_measures)
print(prediction)

#multi_sample
example_measures = np.array([[4,2,1,1,1,2,3,2,1],[8,2,9,6,2,9,1,1,1]])
example_measures = example_measures.reshape(2, -1)
prediction = clf.predict(example_measures)
print(prediction)

#result 2=Benign 4=Malignant

------------------------------------------------------------------------
#ML for breast cancer
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
import warnings
from math import sqrt
from collections import Counter
import pandas as pd
import random
#%matplotlib qt
#style.use('fivethirtyeight')

def k_nearest_neighbors(data, predict, k=3):
    if len(data) >= k:
        warnings.warn('K is set to a value less than total voting groups!')
        
    distances = []
    for group in data:#group of each training data
        for features in data[group]:
            #euclidean_distance = #real distance-predict distance
            euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict)) 
            #linalg.norm=square(sum)=sqrt( (plot1[0]-plot2[0])**2 + (plot1[1]-plot2[1])**2 )#sqrt=square root
            distances.append([euclidean_distance,group]) #group=classification=color
    votes = [i[1] for i in sorted(distances)[:k]] #i[1]=group #[:k]=k last
    print(votes)
    vote_result = Counter(votes).most_common(1)[0][0] #1st[0]into[],2ed[0]r
    return vote_result

#dataset = {'k':[[1,2],[2,3],[3,1]], 'r':[[6,5],[7,7],[8,6]]} #data
#new_features = [5,7] #predict
#[[plt.scatter(ii[0],ii[1],s=100,color=i) for ii in dataset[i]] for i in dataset]
# same as:
##for i in dataset:
##    for ii in dataset[i]:
##        plt.scatter(ii[0],ii[1],s=100,color=i)

df = pd.read_csv('breast-cancer-wisconsin.data.txt')
df.replace('?',-99999, inplace=True)
df.drop(['id'], 1, inplace=True)
full_data = df.astype(float).values.tolist()

random.shuffle(full_data)

test_size = 0.2
train_set = {2:[], 4:[]}#classify 2&4
test_set = {2:[], 4:[]}
train_data = full_data[:-int(test_size*len(full_data))] #0.2*how many samples #[:-X]=(total-X)(record)
test_data = full_data[-int(test_size*len(full_data)):] #reverse #[-X:]=X(record)

for i in train_data:
    train_set[i[-1]].append(i[:-1]) #[i[-1]]=2/4 #(i[:-1])=other number
    

for i in test_data:
    test_set[i[-1]].append(i[:-1]) #[i[-1]]=2/4 #(i[:-1])=other number

correct = 0
total = 0

for group in test_set:#group=2/4
    for data in test_set[group]:
        vote = k_nearest_neighbors(train_set, data, k=5)
        if group == vote:
            correct += 1
        total += 1
print('Accuracy:', correct/total)

##for figure
#plt.scatter(new_features[0], new_features[1], s=100)

#result = k_nearest_neighbors(dataset, new_features)
#plt.scatter(new_features[0], new_features[1], s=100, color = result)  
#plt.show()
